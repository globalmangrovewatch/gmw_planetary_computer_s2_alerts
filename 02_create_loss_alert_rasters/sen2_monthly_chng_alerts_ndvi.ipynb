{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0acd45d3-63a9-4aa5-a4c7-cc2ebcb0dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import GatewayCluster\n",
    "import dask.distributed\n",
    "import planetary_computer\n",
    "from pystac_client import Client\n",
    "import odc.stac\n",
    "import geopandas\n",
    "import numpy\n",
    "import rasterio\n",
    "import xarray\n",
    "import os\n",
    "import json\n",
    "from azure.storage.blob import BlobClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cad40b5-b72e-4ea9-a082-6c2d6d9c609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sen2_vld_msk(scns_xa, bands, qa_pxl_msk=\"SCL\", out_no_data_val=0):\n",
    "    scns_lcl_xa = scns_xa.copy()\n",
    "    for band in bands:\n",
    "        scns_lcl_xa[band].values[\n",
    "            scns_lcl_xa[qa_pxl_msk].values == 0\n",
    "        ] = out_no_data_val  # No Data\n",
    "        scns_lcl_xa[band].values[\n",
    "            scns_lcl_xa[qa_pxl_msk].values == 1\n",
    "        ] = out_no_data_val  # Saturation\n",
    "        scns_lcl_xa[band].values[\n",
    "            scns_lcl_xa[qa_pxl_msk].values == 2\n",
    "        ] = out_no_data_val  # Cast Shadow\n",
    "        scns_lcl_xa[band].values[\n",
    "            scns_lcl_xa[qa_pxl_msk].values == 3\n",
    "        ] = out_no_data_val  # Cloud Shadows\n",
    "        scns_lcl_xa[band].values[\n",
    "            scns_lcl_xa[qa_pxl_msk].values == 8\n",
    "        ] = out_no_data_val  # Cloud Medium Probability\n",
    "        scns_lcl_xa[band].values[\n",
    "            scns_lcl_xa[qa_pxl_msk].values == 9\n",
    "        ] = out_no_data_val  # Cloud High Probability\n",
    "        scns_lcl_xa[band].values[\n",
    "            scns_lcl_xa[qa_pxl_msk].values == 10\n",
    "        ] = out_no_data_val  # Thin Cirrus\n",
    "    return scns_lcl_xa\n",
    "\n",
    "\n",
    "def apply_sen2_offset(sen2_scns_xa, offset=-1000):\n",
    "\n",
    "    # Define the date splitting whether the offset should be applied.\n",
    "    off_date = numpy.datetime64(\"2022-01-25\")\n",
    "    # Get Minimum date in timeseries\n",
    "    time_min = sen2_scns_xa.time.min().values\n",
    "    # Get Maximum date in timeseries\n",
    "    time_max = sen2_scns_xa.time.max().values\n",
    "\n",
    "    # Get the list of variables\n",
    "    bands = list(sen2_scns_xa.data_vars)\n",
    "    # List of all bands for which offset should be applied if present.\n",
    "    s2_img_bands = [\n",
    "        \"B01\",\n",
    "        \"B02\",\n",
    "        \"B03\",\n",
    "        \"B04\",\n",
    "        \"B05\",\n",
    "        \"B06\",\n",
    "        \"B07\",\n",
    "        \"B08\",\n",
    "        \"B8A\",\n",
    "        \"B09\",\n",
    "        \"B10\",\n",
    "        \"B11\",\n",
    "        \"B12\",\n",
    "    ]\n",
    "\n",
    "    if (time_min < off_date) and (time_max > off_date):\n",
    "        # Crosses the offset data and therefore part of the dataset needs offset applying\n",
    "        sen2_scns_xa_pre_off = sen2_scns_xa.sel(time=slice(time_min, off_date))\n",
    "        sen2_scns_xa_post_off = sen2_scns_xa.sel(time=slice(off_date, time_max))\n",
    "        for band in bands:\n",
    "            if band in s2_img_bands:\n",
    "                sen2_scns_xa_post_off[band] = sen2_scns_xa_post_off[band] + offset\n",
    "                sen2_scns_xa_post_off[band].where(sen2_scns_xa_post_off[band] < 0, 0)\n",
    "                sen2_scns_xa_post_off[band].where(\n",
    "                    sen2_scns_xa_post_off[band] > 10000, 0\n",
    "                )\n",
    "        sen2_scns_xa = xarray.concat(\n",
    "            [sen2_scns_xa_pre_off, sen2_scns_xa_post_off], dim=\"time\"\n",
    "        )\n",
    "    elif time_min > off_date:\n",
    "        # All scenes after offset date apply to all\n",
    "        for band in bands:\n",
    "            if band in s2_img_bands:\n",
    "                sen2_scns_xa[band] = sen2_scns_xa[band] + offset\n",
    "                sen2_scns_xa[band].where(sen2_scns_xa[band] < 0, 0)\n",
    "                sen2_scns_xa[band].where(sen2_scns_xa[band] > 10000, 0)\n",
    "    # else: time_max < off_date:\n",
    "    # Do nothing - no offset required\n",
    "    return sen2_scns_xa\n",
    "\n",
    "\n",
    "def get_img_metadata(img_file):\n",
    "    img_data_obj = rasterio.open(img_file)\n",
    "    img_bounds = img_data_obj.bounds\n",
    "    img_bbox = [img_bounds.left, img_bounds.bottom, img_bounds.right, img_bounds.top]\n",
    "    img_x_res, img_y_res  = img_data_obj.res\n",
    "    if img_y_res > 0:\n",
    "        img_y_res = img_y_res * (-1)\n",
    "    img_data_obj = None\n",
    "    return img_bbox, img_x_res, img_y_res\n",
    "\n",
    "\n",
    "def get_img_band_array(img_file, band=1):\n",
    "    img_data_obj = rasterio.open(img_file)\n",
    "    img_arr = img_data_obj.read(band)\n",
    "    img_data_obj = None\n",
    "    return img_arr\n",
    "\n",
    "\n",
    "def find_month_end_date(year, month):\n",
    "    import calendar\n",
    "    cal = calendar.Calendar()\n",
    "    month_days = cal.monthdayscalendar(year, month)\n",
    "    max_day_month = numpy.array(month_days).flatten().max()\n",
    "    return max_day_month\n",
    "\n",
    "\n",
    "def zero_pad_num_str(\n",
    "    num_val: float,\n",
    "    str_len: int = 3,\n",
    "    round_num: bool = False,\n",
    "    round_n_digts: int = 0,\n",
    "    integerise: bool = False,\n",
    "    absolute: bool = False,\n",
    "    gain: float = 1,\n",
    ") -> str:\n",
    "    if absolute:\n",
    "        num_val = abs(num_val)\n",
    "    if round_num:\n",
    "        num_val = round(num_val, round_n_digts)\n",
    "    if integerise:\n",
    "        num_val = int(num_val * gain)\n",
    "\n",
    "    num_str = \"{}\".format(num_val)\n",
    "    num_str = num_str.zfill(str_len)\n",
    "    return num_str\n",
    "\n",
    "\n",
    "def test_asset_urls(signed_items):\n",
    "    chkd_items = list()\n",
    "    for scn_item in signed_items:\n",
    "        assets_present = True\n",
    "        for asset_name in scn_item.assets:\n",
    "            try:\n",
    "                if (\n",
    "                    urllib.request.urlopen(scn_item.assets[asset_name].href).getcode()\n",
    "                    != 200\n",
    "                ):\n",
    "                    assets_present = False\n",
    "                    break\n",
    "            except urllib.error.HTTPError:\n",
    "                assets_present = False\n",
    "                break\n",
    "            time.sleep(0.1)\n",
    "        if assets_present:\n",
    "            chkd_items.append(scn_item)\n",
    "    print(f\"Before: {len(signed_items)}\")\n",
    "    print(f\"After: {len(chkd_items)}\")\n",
    "    return chkd_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec33b661-aa47-463b-932c-3d26d62655a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_recent_alerts(tile, gmw_ext_msk, start_year, end_year, c_year, c_month, out_dir, sas_token_info):\n",
    "    years = numpy.arange(start_year, end_year, 1)\n",
    "    found_tile_alerts = False\n",
    "    found_miss_alerts = False\n",
    "    alerts_year = 0\n",
    "    alerts_month = 0\n",
    "    for year in years:\n",
    "        for month_idx in numpy.arange(0, 12, 1):\n",
    "            month = month_idx + 1\n",
    "            month_end_day = find_month_end_date(year, month)\n",
    "            month_str = zero_pad_num_str(month, str_len = 2, round_num = False, round_n_digts = 0)\n",
    "            month_end_day_str = zero_pad_num_str(month_end_day, str_len = 2, round_num = False, round_n_digts = 0)\n",
    "            date_str = f\"{year}{month_str}\"\n",
    "            if (year == c_year) and (month == c_month):\n",
    "                # Cannot process beyond current month of current year as there will be no data - its the future...\n",
    "                break\n",
    "\n",
    "            out_alerts_img_file = f\"gmw_{tile}_{date_str}_chg_alerts.tif\"\n",
    "            out_alerts_img_file_url = os.path.join(sas_token_info[\"url\"], out_dir, out_alerts_img_file)\n",
    "            out_alerts_img_file_url_signed = f\"{out_alerts_img_file_url}?{sas_token_info['sas_token']}\"\n",
    "            out_alerts_img_exists =  BlobClient.from_blob_url(out_alerts_img_file_url_signed).exists()\n",
    "            \n",
    "            out_meta_img_file = f\"gmw_{tile}_{date_str}_chg_alerts_meta.tif\"\n",
    "            out_meta_img_file_url = os.path.join(sas_token_info[\"url\"], out_dir, out_meta_img_file)\n",
    "            out_meta_img_file_url_signed = f\"{out_meta_img_file_url}?{sas_token_info['sas_token']}\"\n",
    "            out_meta_img_exists =  BlobClient.from_blob_url(out_meta_img_file_url_signed).exists()\n",
    "            \n",
    "            if out_meta_img_exists and out_alerts_img_exists:\n",
    "                found_tile_alerts = True\n",
    "                meta_img_file = out_meta_img_file\n",
    "                alerts_img_file = out_alerts_img_file\n",
    "                alerts_year = year\n",
    "                alerts_month_idx = month_idx\n",
    "            else:\n",
    "                found_miss_alerts = True\n",
    "                break\n",
    "        if found_miss_alerts:\n",
    "            break\n",
    "    \n",
    "    if found_tile_alerts:\n",
    "        start_month_idx = alerts_month_idx + 1\n",
    "        if start_month_idx == 12:\n",
    "            start_month_idx = 0\n",
    "            alerts_year = alerts_year + 1\n",
    "        years = numpy.arange(alerts_year, end_year, 1)\n",
    "        # Define the signed URL\n",
    "        meta_img_file_url = os.path.join(sas_token_info[\"url\"], out_dir, meta_img_file)\n",
    "        meta_img_file_url_signed = f\"{meta_img_file_url}?{sas_token_info['sas_token']}\"\n",
    "        # Read the layers in.\n",
    "        score_arr = get_img_band_array(meta_img_file_url_signed, band=1)\n",
    "        date_year_arr = get_img_band_array(meta_img_file_url_signed, band=2)\n",
    "        date_month_arr = get_img_band_array(meta_img_file_url_signed, band=3)\n",
    "    else:\n",
    "        score_arr = numpy.zeros_like(gmw_ext_msk, dtype=numpy.int16)\n",
    "        date_year_arr = numpy.zeros_like(gmw_ext_msk, dtype=numpy.uint16)\n",
    "        date_month_arr = numpy.zeros_like(gmw_ext_msk, dtype=numpy.uint8)\n",
    "        start_month_idx = 0\n",
    "    \n",
    "    return years, start_month_idx, score_arr, date_year_arr, date_month_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cddeadce-9f71-46d9-bb07-feb827f70edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pccompute.westeurope.cloudapp.azure.com/compute/services/dask-gateway/clusters/prod.1aae2a543dd54657acacdc08fe3cce2c/status\n"
     ]
    }
   ],
   "source": [
    "cluster = GatewayCluster()  # Creates the Dask Scheduler. Might take a minute.\n",
    "cluster.adapt(minimum=4, maximum=24)\n",
    "print(cluster.dashboard_link)\n",
    "\n",
    "client = dask.distributed.Client(cluster, timeout=5)\n",
    "odc.stac.configure_rio(cloud_defaults=True, client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fe53db3-3418-4933-8bef-c3940f65d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb2015b4-69e3-44e4-9700-a66d4c2508be",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [\"B04\", \"B08\", \"SCL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3184931e-9714-47ac-abb3-80641906d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tiles to be processed.\n",
    "tiles_gdf = geopandas.read_file(\"../00_base_data/alert_region_tiles.geojson\")\n",
    "tiles = tiles_gdf[\"tile\"].values\n",
    "# use to indicate range of tiles for batch processing (e.g. 0:100 for the 1st 100 or 400: for tile 400 to the end)\n",
    "#tiles = tiles[0:100]\n",
    "\n",
    "tiles = tiles.tolist()\n",
    "#tiles.remove(\"N27W077\")\n",
    "\n",
    "# Change to just run some test tiles...\n",
    "# tiles = [\"N00E008\", \"N00E009\", \"N00E041\"]\n",
    "\n",
    "n_tiles = len(tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da2ef6b6-6188-400a-93ae-e6a3f95b805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# significantly slows processing, only use when running tiles that produce e.g. http 404 errors, change to True for running separate tiles\n",
    "check_s2_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c75966a5-0572-428d-9155-887f961794e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_info_file = \"/home/jovyan/azure_info.json\"\n",
    "with open(sas_info_file) as f:\n",
    "    sas_token_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470d2875-2aa3-4d05-ad84-a4a382a33077",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = \"monthly_change_imgs_tmp\"\n",
    "if not os.path.exists(tmp_path):\n",
    "    os.mkdir(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44b83e4b-77b8-4d60-8cd8-0ea8435a81d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img_dir = \"monthly_change_imgs\"\n",
    "gmw_base_dir = \"gmw_2018_revised\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50d7b55f-d6b8-40ca-acb3-498a17476636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a list including start_year until (but excluding) end_year\n",
    "start_year = 2019\n",
    "end_year = 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d1c75c6-213e-49c6-82ec-059c94cb8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update when processing new months. c_month calculates until (excluding) that month\n",
    "c_year = 2022\n",
    "c_month = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb056727-9f07-4718-adde-6cc09d6bc313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N00E008: (1 of 3)\n",
      "\tYears: [2022]; Start Month Index: 11\n",
      "\t2022-01-01/2022-12-31: N Items = 274\n",
      "\t2021-01-01/2021-12-31: N Items = 211\n",
      "\t\t2022 - 12\n",
      "N00E009: (2 of 3)\n",
      "\tYears: [2022]; Start Month Index: 11\n",
      "\t2022-01-01/2022-12-31: N Items = 137\n",
      "\t2021-01-01/2021-12-31: N Items = 106\n",
      "\t\t2022 - 12\n",
      "N00E041: (3 of 3)\n",
      "\tYears: [2022]; Start Month Index: 11\n",
      "\t2022-01-01/2022-12-31: N Items = 303\n",
      "\t2021-01-01/2021-12-31: N Items = 421\n",
      "\t\t2022 - 12\n"
     ]
    }
   ],
   "source": [
    "n_tile = 0\n",
    "for tile in tiles:\n",
    "    print(f\"{tile}: ({n_tile+1} of {n_tiles})\")\n",
    "    \n",
    "    # Path of the revised GMW 2018 extent image.\n",
    "    gmw_tile_img_file = f\"gmw_{tile}_2018_alerts_ext.tif\"\n",
    "    gmw_tile_img_url = os.path.join(sas_token_info[\"url\"], gmw_base_dir, gmw_tile_img_file)\n",
    "    gmw_tile_img_url_signed = f\"{gmw_tile_img_url}?{sas_token_info['sas_token']}\"\n",
    "        \n",
    "    # If the GMW 2018 tile does not exist then skip tile and go to the next one.\n",
    "    if not BlobClient.from_blob_url(gmw_tile_img_url_signed).exists():\n",
    "        print(\"\\tNo GMW Extent Tile Available...\")\n",
    "        n_tile += 1\n",
    "        continue\n",
    "    \n",
    "    # Get image tile meta-data\n",
    "    bbox, img_x_res, img_y_res = get_img_metadata(gmw_tile_img_url_signed)\n",
    "    # Get GMW Mask as array\n",
    "    gmw_ext_msk = get_img_band_array(gmw_tile_img_url_signed, band=2)\n",
    "    \n",
    "    # Check there are mangrove pixels within the scene (mangroves == 1)\n",
    "    if numpy.max(gmw_ext_msk) == 0:\n",
    "        print(\"\\tNo Mangrove Pixels in Mask...\")\n",
    "        del gmw_ext_msk\n",
    "        n_tile += 1\n",
    "        continue\n",
    "\n",
    "    # Get the image shape (i.e., number of pixels)\n",
    "    img_shp = gmw_ext_msk.shape\n",
    "\n",
    "    # Define the output image spatial transformation.\n",
    "    out_img_transform = rasterio.transform.Affine(img_x_res, 0.0, bbox[0], 0.0, img_y_res, bbox[3])\n",
    "    \n",
    "    # Find when the latest alerts have been created and continue from any previous alerts\n",
    "    # This is required for if the analysis crashes or when rerun each month for new alerts\n",
    "    years, start_month_idx, score_arr, date_year_arr, date_month_arr = find_recent_alerts(tile, gmw_ext_msk, start_year, end_year, c_year, c_month, out_img_dir, sas_token_info)\n",
    "    print(f\"\\tYears: {years}; Start Month Index: {start_month_idx}\")\n",
    "    \n",
    "    # Skip to the next tile if the tile alerts for the tile have already been processed.\n",
    "    if (len(years) == 1) and (years[0] == c_year) and (start_month_idx == c_month-1):\n",
    "        n_tile += 1\n",
    "        continue\n",
    "    \n",
    "    # Iterate through the years for the analysis\n",
    "    for year in years:\n",
    "        # Date range of the ROI (Beginning of Jan to end of Dec)\n",
    "        chng_time_range = f\"{year}-01-01/{year}-12-31\"\n",
    "        \n",
    "        ####################################################################\n",
    "        # Find the scenes for the year of interest.\n",
    "        chng_search = catalog.search(collections=[\"sentinel-2-l2a\"], bbox=bbox, datetime=chng_time_range, query={\"eo:cloud_cover\": {\"lt\": 50}})\n",
    "        chng_items = chng_search.get_all_items()\n",
    "        n_chng_items = len(chng_items)\n",
    "        print(f\"\\t{chng_time_range}: N Items = {n_chng_items}\")\n",
    "        ####################################################################\n",
    "\n",
    "        ####################################################################\n",
    "        # Read the scenes into dask array structure and make persistant in memory\n",
    "        signed_chng_items = [planetary_computer.sign(item) for item in chng_items]\n",
    "        if check_s2_data:\n",
    "            signed_chng_items = test_asset_urls(signed_chng_items)\n",
    "\n",
    "        sen2_chng_scn_xa = odc.stac.stac_load(\n",
    "            signed_chng_items,\n",
    "            bands=bands,\n",
    "            groupby=\"solar_day\",\n",
    "            chunks={\"time\":12, \"latitude\": 1024, \"longitude\": 1024},\n",
    "            bbox=bbox,\n",
    "            crs=\"EPSG:4326\",\n",
    "            resolution=img_x_res\n",
    "        )\n",
    "\n",
    "        # Comment out for larger datasets which don't fit into memory.\n",
    "        sen2_chng_scn_xa = sen2_chng_scn_xa.persist()\n",
    "        \n",
    "        # Apply Offset\n",
    "        sen2_chng_scn_xa = apply_sen2_offset(sen2_chng_scn_xa)\n",
    "        \n",
    "        # Apply cloud mask to all scenes\n",
    "        sen2_chng_scn_xa = sen2_chng_scn_xa.map_blocks(apply_sen2_vld_msk, kwargs={\"bands\":bands})\n",
    "        ####################################################################\n",
    "\n",
    "        ####################################################################\n",
    "        # 'Clean' up the Red and NIR bands to remove any values less than zero.\"\n",
    "        sen2_chng_scn_xa['B04'] = sen2_chng_scn_xa.B04.where(sen2_chng_scn_xa.B04>0)\n",
    "        sen2_chng_scn_xa['B08'] = sen2_chng_scn_xa.B08.where(sen2_chng_scn_xa.B08>0)\n",
    "\n",
    "        # Valid pixel mask\n",
    "        sen2_chng_vld_xa = sen2_chng_scn_xa.B04>0\n",
    "\n",
    "        # Calculate the NDVI\n",
    "        ndvi_chng_scn_xa = ((sen2_chng_scn_xa.B08-sen2_chng_scn_xa.B04)/(sen2_chng_scn_xa.B08+sen2_chng_scn_xa.B04))\n",
    "        ####################################################################\n",
    "\n",
    "\n",
    "        ####################################################################\n",
    "        # Date range of the year before ROI for checking changes have changed over the last 12 months.\n",
    "        ref_time_range = f\"{year-1}-01-01/{year-1}-12-31\"\n",
    "        # Search for reference scenes from year earlier than ROI.\n",
    "        ref_search = catalog.search(collections=[\"sentinel-2-l2a\"], bbox=bbox, datetime=ref_time_range, query={\"eo:cloud_cover\": {\"lt\": 50}})\n",
    "        ref_items = ref_search.get_all_items()\n",
    "        n_ref_items = len(ref_items)\n",
    "        print(f\"\\t{ref_time_range}: N Items = {n_ref_items}\")\n",
    "        ####################################################################\n",
    "\n",
    "        ####################################################################\n",
    "        # Read the scenes into dask array structure and make persistant in memory\n",
    "        signed_ref_items = [planetary_computer.sign(item) for item in ref_items]\n",
    "        if check_s2_data:\n",
    "            signed_ref_items = test_asset_urls(signed_ref_items)\n",
    "\n",
    "        sen2_ref_scn_xa = odc.stac.stac_load(\n",
    "            signed_ref_items,\n",
    "            bands=bands,\n",
    "            groupby=\"solar_day\",\n",
    "            chunks={\"time\":12, \"latitude\": 1024, \"longitude\": 1024},\n",
    "            bbox=bbox,\n",
    "            crs=\"EPSG:4326\",\n",
    "            resolution=img_x_res\n",
    "        )\n",
    "\n",
    "        # Comment out for larger datasets which don't fit into memory.\n",
    "        sen2_ref_scn_xa = sen2_ref_scn_xa.persist()\n",
    "        \n",
    "        # Apply Offset\n",
    "        sen2_ref_scn_xa = apply_sen2_offset(sen2_ref_scn_xa)\n",
    "        \n",
    "        # Apply cloud mask to all scenes\n",
    "        sen2_ref_scn_xa = sen2_ref_scn_xa.map_blocks(apply_sen2_vld_msk, kwargs={\"bands\":bands})\n",
    "        ####################################################################\n",
    "\n",
    "        ####################################################################\n",
    "        # 'Clean\\' up the Red and NIR bands to remove any values less than zero.\"\n",
    "        sen2_ref_scn_xa['B04'] = sen2_ref_scn_xa.B04.where(sen2_ref_scn_xa.B04>0)\n",
    "        sen2_ref_scn_xa['B08'] = sen2_ref_scn_xa.B08.where(sen2_ref_scn_xa.B08>0)\n",
    "\n",
    "        # Calculate the NDVI\n",
    "        ndvi_ref_scn_xa = ((sen2_ref_scn_xa.B08-sen2_ref_scn_xa.B04)/(sen2_ref_scn_xa.B08+sen2_ref_scn_xa.B04))\n",
    "\n",
    "        # Create monthly summaries\n",
    "        monthly_ref_ndvi_xa = ndvi_ref_scn_xa.resample(time='1MS').median(skipna=True).compute()\n",
    "        ####################################################################\n",
    "        \n",
    "        # If first year start at the starting month which might not be Jan.\n",
    "        if years[0] == year:\n",
    "            months = numpy.arange(start_month_idx, 12, 1)\n",
    "        else:\n",
    "            months = numpy.arange(0, 12, 1)\n",
    "        \n",
    "        # Iterate through the months...\n",
    "        for month_idx in months:\n",
    "            month = month_idx + 1\n",
    "            month_end_day = find_month_end_date(year, month)\n",
    "            month_str = zero_pad_num_str(month, str_len = 2, round_num = False, round_n_digts = 0)\n",
    "            month_end_day_str = zero_pad_num_str(month_end_day, str_len = 2, round_num = False, round_n_digts = 0)\n",
    "            date_str = f\"{year}{month_str}\"\n",
    "            if (year == c_year) and (month == c_month):\n",
    "                # Cannot process beyond current month of current year as there will be no data - its the future...\n",
    "                break\n",
    "            print(f\"\\t\\t{year} - {month_str}\")\n",
    "            \n",
    "            # If there isn't data for a month then set index for last month available so index is within bounds.\n",
    "            if month_idx >= len(monthly_ref_ndvi_xa.time):\n",
    "                month_idx = len(monthly_ref_ndvi_xa.time)-1\n",
    "             \n",
    "            # Take a time slice of the per-scene NDVIs for the current month\n",
    "            ndvi_month_chng_scn_xa = ndvi_chng_scn_xa.sel(time=slice(f\"{year}-{month_str}-01\", f\"{year}-{month_str}-{month_end_day_str}\"))\n",
    "            # Find the pixels of change for the month on a per-scene basis (i.e., NDVI < 0.25 and then difference in NDVI from a year earlier is > 0.15)\n",
    "            monthly_chng_pxls_xa = numpy.logical_and((ndvi_month_chng_scn_xa < 0.25), numpy.abs(ndvi_month_chng_scn_xa - monthly_ref_ndvi_xa.values[month_idx]) > 0.15)\n",
    "            \n",
    "            # Get the valid masks for the current month\n",
    "            monthly_vld_pxls_scn_xa = sen2_chng_vld_xa.sel(time=slice(f\"{year}-{month_str}-01\", f\"{year}-{month_str}-{month_end_day_str}\")) \n",
    "\n",
    "            # Mask the GMW changes and valid masks to the GMW mask.\n",
    "            gmw_monthly_chng_pxls_xa = monthly_chng_pxls_xa.where(gmw_ext_msk == 1)\n",
    "            gmw_monthly_vld_pxls_scn_xa = monthly_vld_pxls_scn_xa.where(gmw_ext_msk == 1)\n",
    "\n",
    "            # Sum the number of changes observed in the scenes\n",
    "            gmw_monthly_chng_pxls_count_xa = gmw_monthly_chng_pxls_xa.sum(dim=\"time\", skipna=True)\n",
    "            # Sum the number of observations.\n",
    "            gmw_monthly_vld_pxls_count_xa = gmw_monthly_vld_pxls_scn_xa.sum(dim=\"time\", skipna=True)\n",
    "            \n",
    "            # Run the compute in dask - before this no analysis actually happens it is just building the analysis graph\n",
    "            dask.compute(gmw_monthly_chng_pxls_count_xa, gmw_monthly_vld_pxls_count_xa)\n",
    "\n",
    "            ####################################################################\n",
    "            # Update the score data values (Add 1 for a change and -1 for no change).\n",
    "            lcl_score_arr = score_arr + (gmw_monthly_chng_pxls_count_xa.values) - (gmw_monthly_vld_pxls_count_xa.values - gmw_monthly_chng_pxls_count_xa.values)\n",
    "            # Any pixels with a score of 5 before the analysis set back to 5 - they have already been identified as a change.\n",
    "            lcl_score_arr[score_arr == 5] = 5\n",
    "\n",
    "            # Update the date layers so that any pixels which have gone back to a score of 0 are reset.\n",
    "            date_year_arr[lcl_score_arr < 1] = 0\n",
    "            date_month_arr[lcl_score_arr < 1] = 0\n",
    "\n",
    "            # For pixels with a score > 0 without a date define, define the date as this is the first observation.\n",
    "            date_year_arr[(lcl_score_arr > 0) & (date_year_arr == 0)] = year\n",
    "            date_month_arr[(lcl_score_arr > 0) & (date_month_arr == 0)] = month\n",
    "\n",
    "            # Identify new confirmed alerts (i.e., the score has now reach 5).\n",
    "            month_alerts_arr = numpy.zeros_like(score_arr, dtype=numpy.uint8)\n",
    "            month_alerts_arr[numpy.logical_and(score_arr<5, lcl_score_arr>4)] = 1\n",
    "\n",
    "            # Cap the score layer at 5 and don't allow scores lower than 0.\n",
    "            lcl_score_arr[lcl_score_arr>5] = 5\n",
    "            lcl_score_arr[lcl_score_arr<1] = 0\n",
    "\n",
    "            # Copy the local score image to replace the score image\n",
    "            score_arr = numpy.copy(lcl_score_arr)\n",
    "            ####################################################################\n",
    "\n",
    "            ####################################################################\n",
    "             # Output file names for the change alerts and associated metadata (i.e., score and date of first change observation).\n",
    "            out_meta_img_file = f\"gmw_{tile}_{date_str}_chg_alerts_meta.tif\"\n",
    "            out_alerts_img_file = f\"gmw_{tile}_{date_str}_chg_alerts.tif\"\n",
    "            \n",
    "            out_meta_img_file_tmp = os.path.join(tmp_path, out_meta_img_file)\n",
    "            out_alerts_img_file_tmp = os.path.join(tmp_path, out_alerts_img_file)\n",
    "            \n",
    "            # Create the output image file for the metadata\n",
    "            with rasterio.open(out_meta_img_file_tmp,\n",
    "                                            'w',\n",
    "                                            driver='COG',\n",
    "                                            height=img_shp[0],\n",
    "                                            width=img_shp[1],\n",
    "                                            count=3,\n",
    "                                            dtype=numpy.uint16,\n",
    "                                            crs='epsg:4326',\n",
    "                                            transform=out_img_transform,\n",
    "                                        ) as out_img_dataset:\n",
    "\n",
    "                # Write output array to the image file\n",
    "                out_img_dataset.write(score_arr, 1)\n",
    "                # Name the image band.\n",
    "                out_img_dataset.set_band_description(1, \"Alerts_Score\")\n",
    "\n",
    "                # Write output array to the image file\n",
    "                out_img_dataset.write(date_year_arr, 2)\n",
    "                # Name the image band.\n",
    "                out_img_dataset.set_band_description(2, \"Alert_Date_Year\")\n",
    "\n",
    "                # Write output array to the image file\n",
    "                out_img_dataset.write(date_month_arr, 3)\n",
    "                # Name the image band.\n",
    "                out_img_dataset.set_band_description(3, \"Alert_Date_Month\")\n",
    "            ####################################################################\n",
    "\n",
    "\n",
    "            ####################################################################\n",
    "            # Create the output image file for the newly identified alerts.\n",
    "            with rasterio.open(out_alerts_img_file_tmp,\n",
    "                                            'w',\n",
    "                                            driver='COG',\n",
    "                                            height=img_shp[0],\n",
    "                                            width=img_shp[1],\n",
    "                                            count=1,\n",
    "                                            dtype=numpy.uint8,\n",
    "                                            crs='epsg:4326',\n",
    "                                            transform=out_img_transform,\n",
    "                                        ) as out_img_dataset:\n",
    "\n",
    "                # Write output array to the image file\n",
    "                out_img_dataset.write(month_alerts_arr, 1)\n",
    "                # Name the image band.\n",
    "                out_img_dataset.set_band_description(1, \"Alerts\")\n",
    "            ####################################################################\n",
    "            \n",
    "            if os.path.exists(out_meta_img_file_tmp):\n",
    "                out_meta_img_file_url = os.path.join(sas_token_info[\"url\"], out_img_dir, out_meta_img_file)\n",
    "                out_meta_img_file_url_signed = f\"{out_meta_img_file_url}?{sas_token_info['sas_token']}\"\n",
    "                blob_client = BlobClient.from_blob_url(out_meta_img_file_url_signed)\n",
    "                with open(out_meta_img_file_tmp, 'rb') as data:\n",
    "                    blob_client.upload_blob(data)\n",
    "                blob_client = None\n",
    "                rasterio.shutil.delete(out_meta_img_file_tmp, driver=\"COG\")\n",
    "                        \n",
    "            if os.path.exists(out_alerts_img_file_tmp):\n",
    "                out_alerts_img_file_url = os.path.join(sas_token_info[\"url\"], out_img_dir, out_alerts_img_file)\n",
    "                out_alerts_img_file_url_signed = f\"{out_alerts_img_file_url}?{sas_token_info['sas_token']}\"\n",
    "                blob_client = BlobClient.from_blob_url(out_alerts_img_file_url_signed)\n",
    "                with open(out_alerts_img_file_tmp, 'rb') as data:\n",
    "                    blob_client.upload_blob(data)\n",
    "                blob_client = None\n",
    "                rasterio.shutil.delete(out_alerts_img_file_tmp, driver=\"COG\")\n",
    "            \n",
    "            # Delete Monthly Objects.\n",
    "            del ndvi_month_chng_scn_xa\n",
    "            del monthly_chng_pxls_xa\n",
    "            del monthly_vld_pxls_scn_xa\n",
    "            del gmw_monthly_chng_pxls_xa\n",
    "            del gmw_monthly_vld_pxls_scn_xa\n",
    "            del gmw_monthly_chng_pxls_count_xa\n",
    "            del gmw_monthly_vld_pxls_count_xa\n",
    "            del lcl_score_arr\n",
    "            del month_alerts_arr\n",
    "        # Delete annual objects\n",
    "        del sen2_chng_scn_xa\n",
    "        del sen2_chng_vld_xa\n",
    "        del ndvi_chng_scn_xa\n",
    "        del sen2_ref_scn_xa\n",
    "        del ndvi_ref_scn_xa\n",
    "        del monthly_ref_ndvi_xa\n",
    "    # Delete tile based arrays\n",
    "    del score_arr\n",
    "    del date_year_arr\n",
    "    del date_month_arr\n",
    "    del gmw_ext_msk\n",
    "    # Increment the tile number for user feedback.\n",
    "    n_tile += 1\n",
    "    # Restart the dask workers to ensure all the memory etc. is cleared.\n",
    "    client.restart(wait_for_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce514953-243e-4d2e-94a5-78793024ee87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/dask_gateway/client.py:1014: RuntimeWarning: coroutine 'rpc.close_rpc' was never awaited\n",
      "  self.scheduler_comm.close_rpc()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# Close the dask cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "473af7e4-5bfb-4ab7-a297-d22d30d1e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if os.path.exists(tmp_path):\n",
    "#    shutil.rmtree(tmp_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
