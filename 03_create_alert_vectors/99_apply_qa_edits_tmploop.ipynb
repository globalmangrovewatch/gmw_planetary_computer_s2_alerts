{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5517fc82-252e-417b-b76f-d3ff1434c130",
   "metadata": {},
   "source": [
    "# Apply QA Edits\n",
    "\n",
    "This notebook downloads the vector alerts for a month and associated QA edits and applies those QA edits writing the output files back to the Azure container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4dc5c3-95fa-4407-a0bc-2259a743cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import datetime\n",
    "import json\n",
    "import shutil\n",
    "import geopandas\n",
    "import pandas\n",
    "from azure.storage.blob import BlobClient, ContainerClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f004e0b-7713-4b07-bb41-41d3e6da540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad_num_str(\n",
    "    num_val: float,\n",
    "    str_len: int = 3,\n",
    "    round_num: bool = False,\n",
    "    round_n_digts: int = 0,\n",
    "    integerise: bool = False,\n",
    "    absolute: bool = False,\n",
    "    gain: float = 1,\n",
    ") -> str:\n",
    "    if absolute:\n",
    "        num_val = abs(num_val)\n",
    "    if round_num:\n",
    "        num_val = round(num_val, round_n_digts)\n",
    "    if integerise:\n",
    "        num_val = int(num_val * gain)\n",
    "\n",
    "    num_str = \"{}\".format(num_val)\n",
    "    num_str = num_str.zfill(str_len)\n",
    "    return num_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae70a3-c9f8-4aa7-b55b-82041d10a681",
   "metadata": {},
   "source": [
    "## Read Azure authentication info and create a local temporary directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c3909-0d45-4249-8eb9-9b4215317e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_info_file = \"/home/jovyan/azure_info.json\"\n",
    "with open(sas_info_file) as f:\n",
    "    sas_token_info = json.load(f)\n",
    "    \n",
    "tmp_dir = \"tmp_lcl\"\n",
    "if not os.path.exists(tmp_dir):\n",
    "    os.mkdir(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f2cda-32a9-46ea-9d2c-1b1d731aa029",
   "metadata": {},
   "source": [
    "## Specify the month and year to be processed\n",
    "\n",
    "### January 2019:\n",
    "```\n",
    "c_month = 1\n",
    "c_year = 2019\n",
    "```\n",
    "### June 2020:\n",
    "```\n",
    "c_month = 6\n",
    "c_year = 2020\n",
    "```\n",
    "### December 2021:\n",
    "```\n",
    "c_month = 12\n",
    "c_year = 2021\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e052a-4e55-42d6-ab38-44ead1ce87a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_year = 2022\n",
    "upload = True\n",
    "overwrite_azure = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766719bb-e98a-4ad5-b3f1-18b88a7d3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_month in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    print(c_month)\n",
    "    c_month_str = zero_pad_num_str(c_month, str_len=2)\n",
    "    \n",
    "    # Find check if alerts vector exists for the month specified and download if it does exist\n",
    "    alerts_vec_lyr = f\"{c_year}_{c_month_str}\"\n",
    "    alerts_vec_file = f\"gmw_alerts_{alerts_vec_lyr}_v1.parquet.gzip\"\n",
    "    alerts_vec_file_url = os.path.join(sas_token_info[\"url\"], \"monthly_alert_vecs\", alerts_vec_file)\n",
    "    alerts_vec_file_url_signed = f\"{alerts_vec_file_url}?{sas_token_info['sas_token']}\"\n",
    "    alerts_vec_blob_client = BlobClient.from_blob_url(alerts_vec_file_url_signed)\n",
    "    if not alerts_vec_blob_client.exists():\n",
    "        raise Exception(\"A vector alerts file does not exist for the month/year specified - have you generated?\")\n",
    "\n",
    "    alerts_vec_lcl_file = os.path.join(tmp_dir, alerts_vec_file)\n",
    "    with open(file=alerts_vec_lcl_file, mode=\"wb\") as download_file:\n",
    "        download_file.write(alerts_vec_blob_client.download_blob().readall())\n",
    "    alerts_vec_blob_client = None\n",
    "\n",
    "    # Read the downloaded vector layer into geopandas\n",
    "    alerts_gdf = geopandas.read_parquet(alerts_vec_lcl_file)\n",
    "    alerts_gdf = alerts_gdf.set_crs(epsg=4326, allow_override=True)\n",
    "    \n",
    "    \n",
    "    # Find check if alerts QA vector exists for the month specified and download if it does exist\n",
    "    alerts_qa_vec_lyr = f\"gmw_alerts_qa_{c_year}{c_month_str}\"\n",
    "    alerts_qa_vec_file = f\"{alerts_qa_vec_lyr}.geojson\"\n",
    "    alerts_qa_vec_file_url = os.path.join(sas_token_info[\"url\"], \"monthly_qa_edit_vecs\", alerts_qa_vec_file)\n",
    "    alerts_qa_vec_file_url_signed = f\"{alerts_qa_vec_file_url}?{sas_token_info['sas_token']}\"\n",
    "    alerts_qa_vec_blob_client = BlobClient.from_blob_url(alerts_qa_vec_file_url_signed)\n",
    "    alerts_qa_exists = alerts_qa_vec_blob_client.exists()\n",
    "    alerts_qa_vec_lcl_file = os.path.join(tmp_dir, alerts_qa_vec_file)\n",
    "    if alerts_qa_exists:  \n",
    "        with open(file=alerts_qa_vec_lcl_file, mode=\"wb\") as download_file:\n",
    "            download_file.write(alerts_qa_vec_blob_client.download_blob().readall())\n",
    "    alerts_qa_vec_blob_client = None\n",
    "    \n",
    "    if alerts_qa_exists:\n",
    "        alerts_qa_gdf = geopandas.read_file(alerts_qa_vec_lcl_file)\n",
    "        alerts_qa_gdf = alerts_qa_gdf.set_crs(epsg=4326, allow_override=True)\n",
    "        n_alerts_before = len(alerts_gdf)\n",
    "\n",
    "        # Add column with unique id for each row.\n",
    "        alerts_gdf[\"uid_tmp\"] = numpy.arange(1, (alerts_gdf.shape[0]) + 1, 1, dtype=int)\n",
    "        # Perform selection\n",
    "        sel_alerts_gdf = geopandas.sjoin(\n",
    "            alerts_gdf, alerts_qa_gdf, how=\"inner\", predicate=\"intersects\"\n",
    "        )\n",
    "        # Remove any duplicate features using the tmp column\n",
    "        sel_alerts_gdf.drop_duplicates(subset=[\"uid_tmp\"], inplace=True)\n",
    "        # Create new column with the selection populated as True.\n",
    "        alerts_gdf[\"rm_qa\"] = alerts_gdf[\"uid_tmp\"].isin(sel_alerts_gdf[\"uid_tmp\"].values)\n",
    "        alerts_gdf = alerts_gdf[~alerts_gdf[\"rm_qa\"]]\n",
    "        # Remove the tmp column\n",
    "        alerts_gdf.drop(columns=[\"uid_tmp\"], inplace=True)\n",
    "        n_alerts_after = len(alerts_gdf)\n",
    "        print(f\"Alerts reduced from {n_alerts_before} to {n_alerts_after}.\")\n",
    "        \n",
    "    if len(alerts_gdf) > 0:\n",
    "        alerts_vec_lyr = f\"{c_year}_{c_month_str}\"\n",
    "\n",
    "        out_alerts_vec_pq_file = f\"gmw_alerts_{alerts_vec_lyr}_qad_v1.parquet.gzip\"\n",
    "        out_alerts_vec_pq_file_path = os.path.join(tmp_dir, out_alerts_vec_pq_file)\n",
    "\n",
    "        out_alerts_vec_file = f\"gmw_alerts_{alerts_vec_lyr}_qad_v1.gpkg\"\n",
    "        out_alerts_vec_file_path = os.path.join(tmp_dir, out_alerts_vec_file)\n",
    "\n",
    "        alerts_gdf.to_file(out_alerts_vec_file_path, layer=alerts_vec_lyr, driver=\"GPKG\")\n",
    "        alerts_gdf.to_parquet(out_alerts_vec_pq_file_path, compression='gzip')\n",
    "\n",
    "        if upload:\n",
    "            if upload and os.path.exists(out_alerts_vec_file_path):\n",
    "                alerts_vec_file_url = os.path.join(sas_token_info[\"url\"], \"monthly_alert_qad_vecs\", out_alerts_vec_file)\n",
    "                alerts_vec_file_url_signed = f\"{alerts_vec_file_url}?{sas_token_info['sas_token']}\"\n",
    "                blob_client = BlobClient.from_blob_url(alerts_vec_file_url_signed)\n",
    "                with open(out_alerts_vec_file_path, 'rb') as data:\n",
    "                    blob_client.upload_blob(data, overwrite=overwrite_azure)\n",
    "                blob_client = None\n",
    "            if upload and os.path.exists(out_alerts_vec_pq_file_path):\n",
    "                alerts_pq_file_url = os.path.join(sas_token_info[\"url\"], \"monthly_alert_qad_vecs\", out_alerts_vec_pq_file)\n",
    "                alerts_pq_file_url_signed = f\"{alerts_pq_file_url}?{sas_token_info['sas_token']}\"\n",
    "                blob_client = BlobClient.from_blob_url(alerts_pq_file_url_signed)\n",
    "                with open(out_alerts_vec_pq_file_path, 'rb') as data:\n",
    "                    blob_client.upload_blob(data, overwrite=overwrite_azure)\n",
    "                blob_client = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c95a0-5224-489d-895b-24e50fa4a46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35a4edea-973f-42ac-b85c-b4bfb7715d1f",
   "metadata": {},
   "source": [
    "## Remove the Temporay Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b965bfb-76ea-4575-a355-434655168b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af584565-c009-43ea-aea3-e22c5f23d2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
